{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ab2929",
   "metadata": {},
   "source": [
    "Neural Network from scratch using no ML libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activations):\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.learning_rate = None\n",
    "\n",
    "        #weights and biases initialization\n",
    "        for i in range(len(layers) - 1):\n",
    "            in_size = layers[i]\n",
    "            out_size = layers[i + 1]\n",
    "            limit = np.sqrt(6 / (in_size + out_size))\n",
    "            w = np.random.uniform(-limit, limit, (out_size, in_size))\n",
    "            b = np.zeros(out_size)\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def _activation(self, x, func):\n",
    "        if func == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif func == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif func == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif func == 'leaky_relu':\n",
    "            return np.where(x > 0, x, x * 0.01)\n",
    "        elif func == 'linear':\n",
    "            return x\n",
    "        elif func == 'softmax':\n",
    "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {func}\")\n",
    "\n",
    "    def _activation_derivative(self, x, func):\n",
    "        if func == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-x))\n",
    "            return s * (1 - s)\n",
    "        elif func == 'tanh':\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "        elif func == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        elif func == 'leaky_relu':\n",
    "            return np.where(x > 0, 1.0, 0.01)\n",
    "        elif func == 'linear':\n",
    "            return np.ones_like(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function for derivative: {func}\")\n",
    "\n",
    "    def _cross_entropy_loss(self, y_true, y_pred):\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "    def _cross_entropy_derivative(self, y_true, y_pred):\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        a = X\n",
    "        activations = [a]\n",
    "        zs = []\n",
    "\n",
    "        for w, b, func in zip(self.weights, self.biases, self.activations):\n",
    "            z = a @ w.T + b\n",
    "            a = self._activation(z, func)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        return activations, zs\n",
    "\n",
    "    def backpropagate(self, X, y, activations, zs, loss_function='cross_entropy_loss'):\n",
    "        delta = self._cross_entropy_derivative(y, activations[-1])\n",
    "\n",
    "        nablaw = [np.zeros_like(w) for w in self.weights]\n",
    "        nablab = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        for l in reversed(range(len(self.weights))):\n",
    "            nablaw[l] = delta.T @ activations[l]\n",
    "            nablab[l] = np.sum(delta, axis=0)\n",
    "\n",
    "            if l != 0:\n",
    "                delta = (delta @ self.weights[l]) * self._activation_derivative(zs[l-1], self.activations[l-1])\n",
    "\n",
    "        return nablaw, nablab\n",
    "\n",
    "    def updateWeights(self, nabla_w, nabla_b, batch_size):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * nabla_w[i] / batch_size\n",
    "            self.biases[i] -= self.learning_rate * nabla_b[i] / batch_size\n",
    "\n",
    "    def learn(self, X_train, y_train, epochs, learning_rate, batch_size, verbose=True):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            for start in range(0, X_train.shape[0], batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_X = X_train[start:end]\n",
    "                batch_y = y_train[start:end]\n",
    "\n",
    "                activations, zs = self.feedforward(batch_X)\n",
    "                nabla_w, nabla_b = self.backpropagate(batch_X, batch_y, activations, zs)\n",
    "                self.updateWeights(nabla_w, nabla_b, batch_X.shape[0])\n",
    "\n",
    "            if verbose and epoch % 5 == 0:\n",
    "                preds, _ = self.feedforward(X_train)\n",
    "                loss = self._cross_entropy_loss(y_train, preds[-1])\n",
    "                print(f\"Epoch {epoch} - Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.feedforward(X)\n",
    "        return activations[-1]\n",
    "\n",
    "def to_one_hot(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def normalize(X):\n",
    "    X = X.astype(np.float32)\n",
    "    X -= X.mean(axis=0)\n",
    "    X /= (X.std(axis=0) + 1e-8)\n",
    "    return X\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "    pred_labels = np.argmax(y_pred, axis=1)\n",
    "    return np.mean(true_labels == pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699e05c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (286, 41), Labels shape: (286,)\n",
      "False True 0.21865853658536585 0.4133364016920263\n",
      "Epoch 5 - Loss: 0.6820\n",
      "Epoch 10 - Loss: 0.6819\n",
      "Epoch 15 - Loss: 0.6817\n",
      "Epoch 20 - Loss: 0.6815\n",
      "Epoch 25 - Loss: 0.6813\n",
      "Epoch 30 - Loss: 0.6812\n",
      "Epoch 35 - Loss: 0.6810\n",
      "Epoch 40 - Loss: 0.6808\n",
      "Epoch 45 - Loss: 0.6806\n",
      "Epoch 50 - Loss: 0.6804\n",
      "Epoch 55 - Loss: 0.6802\n",
      "Epoch 60 - Loss: 0.6800\n",
      "Epoch 65 - Loss: 0.6798\n",
      "Epoch 70 - Loss: 0.6796\n",
      "Epoch 75 - Loss: 0.6794\n",
      "Epoch 80 - Loss: 0.6792\n",
      "Epoch 85 - Loss: 0.6790\n",
      "Epoch 90 - Loss: 0.6789\n",
      "Epoch 95 - Loss: 0.6788\n",
      "Epoch 100 - Loss: 0.6787\n",
      "Test Accuracy: 56.98%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCurrent dataset is rather small, with the current batch size of 32, batches may have high variance which leads to overfitting.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neural_network import NeuralNetwork, normalize, to_one_hot, accuracy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "data = pd.read_csv('breastfinal.csv')\n",
    "\n",
    "X = data.iloc[:, :-1].values \n",
    "y = data.iloc[:, -1].values \n",
    "\n",
    "print(f\"Data shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(np.min(Xtrain), np.max(Xtrain), np.mean(Xtrain), np.std(Xtrain))\n",
    "#normalize features if nonbinary dataset\n",
    "\n",
    "#Xtrain = normalize(Xtrain)  # or use normalize(X_train) if you want zero mean\n",
    "#Xtest = normalize(Xtest)\n",
    "\n",
    "numclasses = len(np.unique(y))\n",
    "unique_labels = np.unique(y)\n",
    "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "ytrain = np.array([label_to_int[label] for label in ytrain])\n",
    "ytest = np.array([label_to_int[label] for label in ytest])\n",
    "\n",
    "ytrainoh = to_one_hot(ytrain, numclasses)\n",
    "ytestoh = to_one_hot(ytest, numclasses)\n",
    "\n",
    "#building the neural network\n",
    "#Xtrain.shape[1]  # Number of features\n",
    "#consider expanding hidden layers to 32, 32 or 64, 64 for more complex datasets\n",
    "hiddenlayer1 = 16  #number of neurons first hidden layer\n",
    "hiddenlayer2 = 16  #number of neurons in second hidden layer\n",
    "nn = NeuralNetwork([Xtrain.shape[1], hiddenlayer1, hiddenlayer2, numclasses], ['relu', 'relu', 'softmax'])\n",
    "nn.learn(Xtrain, ytrainoh, epochs=100, learning_rate=0.001, batch_size=32)\n",
    "predictions = nn.predict(Xtest)\n",
    "print(f\"Test Accuracy: {accuracy(ytestoh, predictions) * 100:.2f}%\")\n",
    "\n",
    "'''\n",
    "Current dataset is rather small, with the current batch size of 32, batches may have high variance which leads to overfitting.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
